import os
import time
import json
import logging
import redis
from typing import List, Dict, Any, Tuple

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# é…ç½®
REDIS_HOST = os.getenv("REDIS_HOST", "analytics_redis")
REDIS_PORT = int(os.getenv("REDIS_PORT", 6379))
REDIS_STREAM = os.getenv("REDIS_STREAM", "sessions_stream")
CONSUMER_GROUP = os.getenv("CONSUMER_GROUP", "analytics_workers")
CONSUMER_NAME = os.getenv("CONSUMER_NAME", "worker-1")
BATCH_SIZE = int(os.getenv("BATCH_SIZE", 100))
BLOCK_MS = int(os.getenv("BLOCK_MS", 5000))  # 5 ç§’

# é€£æ¥ Redis
redis_client = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=False)

logging.info(f"Analytics Worker starting...")
logging.info(f"Redis: {REDIS_HOST}:{REDIS_PORT}")
logging.info(f"Stream: {REDIS_STREAM}")
logging.info(f"Consumer Group: {CONSUMER_GROUP}")
logging.info(f"Batch Size: {BATCH_SIZE}")


def create_consumer_group():
    """å‰µå»ºæ¶ˆè²»è€…çµ„ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰"""
    try:
        # å˜—è©¦å‰µå»ºæ¶ˆè²»è€…çµ„ï¼Œå¾æœ€æ–°æ¶ˆæ¯é–‹å§‹
        redis_client.xgroup_create(REDIS_STREAM, CONSUMER_GROUP, id='$', mkstream=True)
        logging.info(f"âœ… Created consumer group: {CONSUMER_GROUP}")
    except redis.ResponseError as e:
        if "BUSYGROUP" in str(e):
            logging.info(f"Consumer group already exists: {CONSUMER_GROUP}")
        else:
            logging.error(f"Error creating consumer group: {e}")
            raise


def process_batch(messages: List[Tuple[bytes, Dict[bytes, bytes]]]) -> int:
    """
    è™•ç†ä¸€æ‰¹æ¶ˆæ¯

    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨ [(msg_id, {b'data': b'...'}), ...]

    Returns:
        int: è™•ç†æˆåŠŸçš„æ•¸é‡
    """
    from normalizer import normalize_session, validate_session
    from enricher import enrich_session
    from evaluator import evaluate_session
    from loader import save_session, update_daily_summary

    processed = 0

    for msg_id, msg_data in messages:
        try:
            # === 1. è§£æè³‡æ–™ ===
            data_bytes = msg_data.get(b'data', b'{}')
            session = json.loads(data_bytes)

            sess_uuid = session.get('sess_uuid', 'unknown')

            # === 2. æ­£è¦åŒ– ===
            normalized_session = normalize_session(session)

            # é©—è­‰è³‡æ–™å®Œæ•´æ€§
            is_valid, error_msg = validate_session(normalized_session)
            if not is_valid:
                logging.warning(f"âš ï¸  Invalid session {sess_uuid}: {error_msg}")
                # ä»ç„¶ ACKï¼Œä½†æ¨™è¨˜ç‚ºç„¡æ•ˆ
                redis_client.xack(REDIS_STREAM, CONSUMER_GROUP, msg_id)
                continue

            # === 3. è±å¯ŒåŒ– ===
            enriched_session = enrich_session(normalized_session)

            # === 4. è©•ä¼° ===
            evaluated_session = evaluate_session(enriched_session)

            # === 5. ä¿å­˜ ===
            saved = save_session(evaluated_session)

            if saved:
                # è¨˜éŒ„é—œéµè³‡è¨Š
                risk_score = evaluated_session.get('risk_score', 0)
                threat_level = evaluated_session.get('threat_level', 'INFO')
                alert_level = evaluated_session.get('alert_level', 'INFO')

                logging.info(
                    f"âœ… Processed {sess_uuid} | Risk: {risk_score}/100 | "
                    f"Threat: {threat: {threat_level} | Alert: {alert_level}"
                )

                processed += 1

                # ACK æ¶ˆæ¯ï¼ˆæ¨™è¨˜ç‚ºå·²è™•ç†ï¼‰
                redis_client.xack(REDIS_STREAM, CONSUMER_GROUP, msg_id)
            else:
                logging.error(f"âŒ Failed to save session {sess_uuid}")
                # ä¸ ACKï¼Œå…è¨±é‡è©¦

        except Exception as e:
            logging.error(f"âŒ Error processing message {msg_id}: {e}", exc_info=True)
            # ä¸ ACK å¤±æ•—çš„æ¶ˆæ¯ï¼Œä¹‹å¾Œå¯ä»¥é‡è©¦

    # åœ¨è™•ç†å®Œæ‰¹æ¬¡å¾Œæ›´æ–°æ¯æ—¥æ‘˜è¦
    if processed > 0:
        today = datetime.utcnow().strftime("%Y-%m-%d")
        update_daily_summary(today)

    return processed


def main_loop():
    """ä¸»å¾ªç’°ï¼šæŒçºŒæ¶ˆè²»æ¶ˆæ¯"""
    create_consumer_group()

    last_id = '>'  # > è¡¨ç¤ºåªè®€å–æ–°æ¶ˆæ¯
    total_processed = 0

    logging.info(f"ğŸš€ Worker started, waiting for messages...")

    while True:
        try:
            # æ‰¹æ¬¡è®€å–æ¶ˆæ¯
            # XREADGROUP æœƒé˜»å¡ç›´åˆ°æœ‰æ–°æ¶ˆæ¯æˆ–è¶…æ™‚
            messages = redis_client.xreadgroup(
                CONSUMER_GROUP,
                CONSUMER_NAME,
                {REDIS_STREAM: last_id},
                count=BATCH_SIZE,
                block=BLOCK_MS
            )

            if messages:
                # messages æ ¼å¼: [(stream_name, [(msg_id, msg_data), ...])]
                for stream_name, stream_messages in messages:
                    if stream_messages:
                        logging.info(f"ğŸ“¦ Received batch of {len(stream_messages)} messages")

                        # è™•ç†æ‰¹æ¬¡
                        processed = process_batch(stream_messages)
                        total_processed += processed

                        logging.info(f"âœ… Processed {processed}/{len(stream_messages)} messages (Total: {total_processed})")

            else:
                # è¶…æ™‚ï¼Œæ²’æœ‰æ–°æ¶ˆæ¯
                logging.debug(f"No new messages, waiting...")

        except redis.RedisError as e:
            logging.error(f"âŒ Redis error: {e}")
            time.sleep(5)  # éŒ¯èª¤æ™‚ç­‰å¾… 5 ç§’å¾Œé‡è©¦

        except KeyboardInterrupt:
            logging.info("âš ï¸  Worker stopping...")
            break

        except Exception as e:
            logging.error(f"âŒ Unexpected error: {e}", exc_info=True)
            time.sleep(5)


if __name__ == "__main__":
    main_loop()
