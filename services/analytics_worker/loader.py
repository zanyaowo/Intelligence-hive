"""
è³‡æ–™æŒä¹…åŒ–æ¨¡çµ„

è² è²¬å°‡è™•ç†å¾Œçš„è³‡æ–™ä¿å­˜åˆ°æª”æ¡ˆç³»çµ±æˆ–è³‡æ–™åº«
"""

import os
import json
import logging
from typing import Dict, Any, List
from datetime import datetime
from pathlib import Path

logger = logging.getLogger(__name__)

# é…ç½®
DATA_DIR = os.getenv("DATA_DIR", "/app/data")
OUTPUT_FORMAT = os.getenv("OUTPUT_FORMAT", "jsonl")  # jsonl, json, or database
BATCH_WRITE = os.getenv("BATCH_WRITE", "false").lower() == "true"


def save_session(session: Dict[str, Any]) -> bool:
    """
    ä¿å­˜å–®å€‹ session è³‡æ–™

    Args:
        session: å®Œæ•´è™•ç†å¾Œçš„ session è³‡æ–™

    Returns:
        bool: æ˜¯å¦æˆåŠŸä¿å­˜
    """
    try:
        if OUTPUT_FORMAT == "jsonl":
            return save_to_jsonl(session)
        elif OUTPUT_FORMAT == "json":
            return save_to_json(session)
        elif OUTPUT_FORMAT == "database":
            return save_to_database(session)
        else:
            logger.warning(f"Unknown output format: {OUTPUT_FORMAT}, defaulting to JSONL")
            return save_to_jsonl(session)

    except Exception as e:
        logger.error(f"âŒ Error saving session {session.get('sess_uuid', 'unknown')}: {e}", exc_info=True)
        return False


def save_to_jsonl(session: Dict[str, Any]) -> bool:
    """
    ä¿å­˜ç‚º JSONL æ ¼å¼ï¼ˆæ¯è¡Œä¸€å€‹ JSON ç‰©ä»¶ï¼‰

    æª”æ¡ˆçµ„ç¹”ï¼š
    - data/processed/YYYY-MM-DD/sessions.jsonl
    - data/alerts/YYYY-MM-DD/high_risk.jsonl (é«˜é¢¨éšªè­¦å ±)
    """
    try:
        # å»ºç«‹ç›®éŒ„çµæ§‹
        today = datetime.utcnow().strftime("%Y-%m-%d")
        processed_dir = Path(DATA_DIR) / "processed" / today
        processed_dir.mkdir(parents=True, exist_ok=True)

        # ä¸»æª”æ¡ˆï¼šæ‰€æœ‰ session
        main_file = processed_dir / "sessions.jsonl"

        with open(main_file, "a", encoding="utf-8") as f:
            json.dump(session, f, ensure_ascii=False)
            f.write("\n")

        # å¦‚æœæ˜¯é«˜é¢¨éšªï¼Œé¡å¤–ä¿å­˜åˆ°è­¦å ±æª”æ¡ˆ
        alert_level = session.get('alert_level', 'INFO')
        if alert_level in ['CRITICAL', 'HIGH']:
            alerts_dir = Path(DATA_DIR) / "alerts" / today
            alerts_dir.mkdir(parents=True, exist_ok=True)

            alert_file = alerts_dir / f"{alert_level.lower()}_alerts.jsonl"

            with open(alert_file, "a", encoding="utf-8") as f:
                json.dump(session, f, ensure_ascii=False)
                f.write("\n")

        logger.debug(f"âœ… Saved session {session.get('sess_uuid', 'unknown')} to JSONL")
        return True

    except Exception as e:
        logger.error(f"âŒ Error saving to JSONL: {e}", exc_info=True)
        return False


def save_to_json(session: Dict[str, Any]) -> bool:
    """
    ä¿å­˜ç‚ºç¨ç«‹ JSON æª”æ¡ˆ

    æª”æ¡ˆçµ„ç¹”ï¼š
    - data/processed/YYYY-MM-DD/{sess_uuid}.json
    """
    try:
        today = datetime.utcnow().strftime("%Y-%m-%d")
        processed_dir = Path(DATA_DIR) / "processed" / today
        processed_dir.mkdir(parents=True, exist_ok=True)

        sess_uuid = session.get('sess_uuid', 'unknown')
        filename = processed_dir / f"{sess_uuid}.json"

        with open(filename, "w", encoding="utf-8") as f:
            json.dump(session, f, ensure_ascii=False, indent=2)

        logger.debug(f"âœ… Saved session {sess_uuid} to JSON")
        return True

    except Exception as e:
        logger.error(f"âŒ Error saving to JSON: {e}", exc_info=True)
        return False


def save_to_database(session: Dict[str, Any]) -> bool:
    """
    ä¿å­˜åˆ°è³‡æ–™åº«ï¼ˆPostgreSQL/MongoDB/Elasticsearchï¼‰

    é€™æ˜¯ä¸€å€‹å ä½å¯¦ä½œï¼Œå¯¦éš›ä½¿ç”¨æ™‚éœ€è¦é…ç½®è³‡æ–™åº«é€£æ¥

    æœªä¾†å¯¦ä½œï¼š
    - PostgreSQL: é—œè¯å¼è³‡æ–™åº«ï¼Œé©åˆçµæ§‹åŒ–æŸ¥è©¢
    - MongoDB: æ–‡ä»¶è³‡æ–™åº«ï¼Œé©åˆéˆæ´»çš„ schema
    - Elasticsearch: æœå°‹å¼•æ“ï¼Œé©åˆå…¨æ–‡æœå°‹å’Œåˆ†æ
    """
    try:
        # TODO: å¯¦ä½œè³‡æ–™åº«ä¿å­˜é‚è¼¯
        # ç¯„ä¾‹ï¼š
        # import psycopg2
        # conn = psycopg2.connect(DATABASE_URL)
        # cursor = conn.cursor()
        # cursor.execute("INSERT INTO sessions (...) VALUES (...)")
        # conn.commit()

        logger.warning("Database storage not implemented yet, falling back to JSONL")
        return save_to_jsonl(session)

    except Exception as e:
        logger.error(f"âŒ Error saving to database: {e}", exc_info=True)
        return False


def save_statistics(sessions: List[Dict[str, Any]]) -> bool:
    """
    ä¿å­˜çµ±è¨ˆæ‘˜è¦

    ç”Ÿæˆæ¯æ—¥çµ±è¨ˆå ±å‘Šï¼š
    - ç¸½ session æ•¸
    - æ”»æ“Šé¡å‹åˆ†å¸ƒ
    - é¢¨éšªç­‰ç´šåˆ†å¸ƒ
    - TOP æ”»æ“Šä¾†æº IP
    - å¨è„…è¶¨å‹¢
    """
    try:
        if not sessions:
            return True

        today = datetime.utcnow().strftime("%Y-%m-%d")
        stats_dir = Path(DATA_DIR) / "statistics" / today
        stats_dir.mkdir(parents=True, exist_ok=True)

        # è¨ˆç®—çµ±è¨ˆæ•¸æ“š
        stats = {
            'date': today,
            'total_sessions': len(sessions),
            'attack_type_distribution': {},
            'threat_level_distribution': {},
            'risk_score_distribution': {
                'critical': 0,
                'high': 0,
                'medium': 0,
                'low': 0,
                'info': 0
            },
            'top_source_ips': {},
            'top_user_agents': {},
            'alert_counts': {
                'CRITICAL': 0,
                'HIGH': 0,
                'MEDIUM': 0,
                'LOW': 0,
                'INFO': 0
            },
            'average_risk_score': 0.0,
            'requires_review_count': 0
        }

        total_risk = 0

        for session in sessions:
            # æ”»æ“Šé¡å‹
            for attack_type in session.get('attack_types', []):
                stats['attack_type_distribution'][attack_type] = \
                    stats['attack_type_distribution'].get(attack_type, 0) + 1

            # å¨è„…ç­‰ç´š
            threat_level = session.get('threat_level', 'INFO')
            stats['threat_level_distribution'][threat_level] = \
                stats['threat_level_distribution'].get(threat_level, 0) + 1

            # é¢¨éšªåˆ†æ•¸
            risk_score = session.get('risk_score', 0)
            total_risk += risk_score

            if risk_score >= 70:
                stats['risk_score_distribution']['critical'] += 1
            elif risk_score >= 50:
                stats['risk_score_distribution']['high'] += 1
            elif risk_score >= 30:
                stats['risk_score_distribution']['medium'] += 1
            elif risk_score >= 15:
                stats['risk_score_distribution']['low'] += 1
            else:
                stats['risk_score_distribution']['info'] += 1

            # ä¾†æº IP
            peer_ip = session.get('peer_ip', 'unknown')
            stats['top_source_ips'][peer_ip] = \
                stats['top_source_ips'].get(peer_ip, 0) + 1

            # User Agent
            user_agent = session.get('user_agent', 'unknown')
            stats['top_user_agents'][user_agent] = \
                stats['top_user_agents'].get(user_agent, 0) + 1

            # è­¦å ±ç­‰ç´š
            alert_level = session.get('alert_level', 'INFO')
            stats['alert_counts'][alert_level] = \
                stats['alert_counts'].get(alert_level, 0) + 1

            # éœ€è¦å¯©æŸ¥
            if session.get('requires_review', False):
                stats['requires_review_count'] += 1

        # å¹³å‡é¢¨éšªåˆ†æ•¸
        stats['average_risk_score'] = round(total_risk / len(sessions), 2)

        # æ’åº TOP 10
        stats['top_source_ips'] = dict(sorted(
            stats['top_source_ips'].items(),
            key=lambda x: x[1],
            reverse=True
        )[:10])

        stats['top_user_agents'] = dict(sorted(
            stats['top_user_agents'].items(),
            key=lambda x: x[1],
            reverse=True
        )[:10])

        # ä¿å­˜çµ±è¨ˆå ±å‘Š
        stats_file = stats_dir / "summary.json"
        with open(stats_file, "w", encoding="utf-8") as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ“Š Saved statistics: {stats['total_sessions']} sessions, avg risk: {stats['average_risk_score']}")
        return True

    except Exception as e:
        logger.error(f"âŒ Error saving statistics: {e}", exc_info=True)
        return False


def save_threat_intelligence_feed(sessions: List[Dict[str, Any]]) -> bool:
    """
    ç”Ÿæˆå¨è„…æƒ…å ± Feed

    æ ¼å¼ï¼š
    - IP é»‘åå–®
    - æ”»æ“Šæ¨¡å¼ç°½å
    - IOC (Indicators of Compromise)
    """
    try:
        if not sessions:
            return True

        today = datetime.utcnow().strftime("%Y-%m-%d")
        intel_dir = Path(DATA_DIR) / "threat_intelligence" / today
        intel_dir.mkdir(parents=True, exist_ok=True)

        # æ”¶é›†å¨è„…æƒ…å ±
        malicious_ips = set()
        attack_signatures = set()
        malicious_user_agents = set()
        malicious_payloads = []

        for session in sessions:
            # åªæ”¶é›†é«˜é¢¨éšªçš„æƒ…å ±
            risk_score = session.get('risk_score', 0)
            if risk_score < 50:
                continue

            # æƒ¡æ„ IP
            peer_ip = session.get('peer_ip')
            if peer_ip and peer_ip != '0.0.0.0':
                malicious_ips.add(peer_ip)

            # æ”»æ“Šç°½å
            attack_patterns = session.get('attack_patterns', {})
            signature = attack_patterns.get('pattern_signature', '')
            if signature:
                attack_signatures.add(signature)

            # æƒæå·¥å…·
            ua_info = session.get('user_agent_info', {})
            if ua_info.get('is_scanner', False):
                user_agent = session.get('user_agent')
                if user_agent:
                    malicious_user_agents.add(user_agent)

            # æƒ¡æ„ Payload (sample)
            payload_analysis = session.get('payload_analysis', {})
            suspicious_patterns = payload_analysis.get('suspicious_patterns', [])
            if suspicious_patterns:
                for path in session.get('paths', [])[:3]:  # åªå–å‰3å€‹
                    malicious_payloads.append({
                        'path': path.get('path'),
                        'method': path.get('method'),
                        'attack_type': path.get('attack_type'),
                        'patterns': suspicious_patterns
                    })

        # ä¿å­˜ IP é»‘åå–®
        ip_blacklist_file = intel_dir / "malicious_ips.txt"
        with open(ip_blacklist_file, "w", encoding="utf-8") as f:
            for ip in sorted(malicious_ips):
                f.write(f"{ip}\n")

        # ä¿å­˜æ”»æ“Šç°½å
        signatures_file = intel_dir / "attack_signatures.txt"
        with open(signatures_file, "w", encoding="utf-8") as f:
            for sig in sorted(attack_signatures):
                f.write(f"{sig}\n")

        # ä¿å­˜å¨è„…æƒ…å ±æ‘˜è¦
        intel_summary = {
            'date': today,
            'malicious_ips_count': len(malicious_ips),
            'malicious_ips': list(malicious_ips),
            'attack_signatures_count': len(attack_signatures),
            'attack_signatures': list(attack_signatures),
            'malicious_user_agents': list(malicious_user_agents),
            'sample_payloads': malicious_payloads[:20]  # åªä¿å­˜ 20 å€‹æ¨£æœ¬
        }

        intel_file = intel_dir / "threat_intelligence.json"
        with open(intel_file, "w", encoding="utf-8") as f:
            json.dump(intel_summary, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ”’ Saved threat intelligence: {len(malicious_ips)} IPs, {len(attack_signatures)} signatures")
        return True

    except Exception as e:
        logger.error(f"âŒ Error saving threat intelligence: {e}", exc_info=True)
        return False


def cleanup_old_data(days_to_keep: int = 30) -> bool:
    """
    æ¸…ç†èˆŠè³‡æ–™

    Args:
        days_to_keep: ä¿ç•™å¤©æ•¸

    Returns:
        bool: æ˜¯å¦æˆåŠŸæ¸…ç†
    """
    try:
        base_dir = Path(DATA_DIR)

        if not base_dir.exists():
            return True

        current_date = datetime.utcnow()

        # éæ­·æ‰€æœ‰æ—¥æœŸç›®éŒ„
        for category in ['processed', 'alerts', 'statistics', 'threat_intelligence']:
            category_dir = base_dir / category

            if not category_dir.exists():
                continue

            for date_dir in category_dir.iterdir():
                if not date_dir.is_dir():
                    continue

                try:
                    # è§£ææ—¥æœŸ
                    dir_date = datetime.strptime(date_dir.name, "%Y-%m-%d")
                    age_days = (current_date - dir_date).days

                    # åˆªé™¤éæœŸç›®éŒ„
                    if age_days > days_to_keep:
                        import shutil
                        shutil.rmtree(date_dir)
                        logger.info(f"ğŸ—‘ï¸  Deleted old data: {date_dir}")

                except ValueError:
                    # ä¸æ˜¯æ—¥æœŸæ ¼å¼çš„ç›®éŒ„ï¼Œè·³é
                    continue

        return True

    except Exception as e:
        logger.error(f"âŒ Error cleaning up old data: {e}", exc_info=True)
        return False


def get_storage_stats() -> Dict[str, Any]:
    """ç²å–å„²å­˜çµ±è¨ˆè³‡è¨Š"""
    try:
        base_dir = Path(DATA_DIR)

        if not base_dir.exists():
            return {'error': 'Data directory does not exist'}

        stats = {
            'total_size_mb': 0,
            'file_counts': {},
            'latest_data_date': None
        }

        # è¨ˆç®—å¤§å°å’Œæª”æ¡ˆæ•¸
        for root, dirs, files in os.walk(base_dir):
            for file in files:
                file_path = Path(root) / file
                stats['total_size_mb'] += file_path.stat().st_size / (1024 * 1024)

                ext = file_path.suffix
                stats['file_counts'][ext] = stats['file_counts'].get(ext, 0) + 1

        stats['total_size_mb'] = round(stats['total_size_mb'], 2)

        return stats

    except Exception as e:
        logger.error(f"âŒ Error getting storage stats: {e}")
        return {'error': str(e)}
